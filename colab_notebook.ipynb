{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabula-8B VA Cause of Death Prediction on Google Colab\n",
    "\n",
    "This notebook runs the Tabula-8B model on Google Colab with GPU acceleration for fast inference.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better)\n",
    "- PHMRC dataset file\n",
    "- ~20GB free space for model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup GPU Runtime\n",
    "\n",
    "**IMPORTANT**: Before running this notebook:\n",
    "1. Go to `Runtime` → `Change runtime type`\n",
    "2. Select `GPU` as Hardware accelerator (T4 or better)\n",
    "3. Click `Save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"❌ No GPU detected! Please enable GPU runtime:\")\n",
    "    print(\"   Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/cliu238/tabula-8b-va-prediction.git\n",
    "%cd tabula-8b-va-prediction\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers torch accelerate datasets pandas numpy scikit-learn tqdm python-dotenv pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload PHMRC Data\n",
    "\n",
    "Upload your PHMRC CSV file when prompted. The file should be named:\n",
    "`IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data/raw/PHMRC', exist_ok=True)\n",
    "\n",
    "print(\"Please upload the PHMRC adult dataset CSV file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded file to correct location\n",
    "for filename in uploaded.keys():\n",
    "    if 'ADULT' in filename:\n",
    "        !mv \"{filename}\" data/raw/PHMRC/\n",
    "        print(f\"✅ Moved {filename} to data/raw/PHMRC/\")\n",
    "\n",
    "# Verify file exists\n",
    "!ls -la data/raw/PHMRC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Preprocess PHMRC Data\n\nThis step cleans and prepares the raw PHMRC data for Tabula-8B model input."
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# Create the preprocessor\nfrom src.data.preprocessor import PHMRCPreprocessor\n\nprint(\"Preprocessing PHMRC data...\")\nprint(\"=\"*60)\n\n# Initialize preprocessor\npreprocessor = PHMRCPreprocessor()\n\n# Load raw data\nraw_data_path = 'data/raw/PHMRC/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv'\ndf_raw = preprocessor.load_data(raw_data_path)\n\nprint(f\"\\nRaw data shape: {df_raw.shape}\")\nprint(f\"Columns: {df_raw.shape[1]}\")\nprint(f\"Records: {df_raw.shape[0]}\")\n\n# Preprocess the data\ndf_processed = preprocessor.preprocess(df_raw, keep_target=True)\n\nprint(f\"\\nProcessed data shape: {df_processed.shape}\")\nprint(f\"Features after preprocessing: {df_processed.shape[1] - 1}\")  # -1 for target\n\n# Check target distribution\nif 'cause_of_death' in df_processed.columns:\n    print(f\"\\nUnique causes of death: {df_processed['cause_of_death'].nunique()}\")\n    print(\"\\nTop 10 causes:\")\n    print(df_processed['cause_of_death'].value_counts().head(10))\n\n# Save preprocessed data\nprocessed_path = 'data/preprocessed/phmrc_processed.csv'\nPath('data/preprocessed').mkdir(parents=True, exist_ok=True)\ndf_processed.to_csv(processed_path, index=False)\nprint(f\"\\n✅ Preprocessed data saved to {processed_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 5: Create GPU-Optimized Model with Proper CSV Formatting"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%%writefile run_colab.py\n#!/usr/bin/env python\n\"\"\"\nColab-optimized script for Tabula-8B VA prediction with GPU support\nUses proper CSV formatting for Tabula-8B model\n\"\"\"\n\nimport sys\nimport torch\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\n\n# Add src to path\nsys.path.insert(0, str(Path.cwd()))\n\nfrom src.data.preprocessor import PHMRCPreprocessor\nfrom src.data.tabula_formatter import TabulaVAFormatter\n\ndef load_model_gpu():\n    \"\"\"Load Tabula-8B with GPU optimization.\"\"\"\n    print(\"Loading Tabula-8B model on GPU...\")\n    \n    model_name = \"mlfoundations/tabula-8b\"\n    \n    # Load with GPU and half precision for speed\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True\n    )\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    print(f\"✅ Model loaded on {torch.cuda.get_device_name(0)}\")\n    return model, tokenizer\n\ndef predict_batch_gpu(model, tokenizer, df_batch, formatter, batch_size=4):\n    \"\"\"Run batch predictions on GPU using CSV format.\"\"\"\n    predictions = []\n    device = next(model.parameters()).device\n    \n    # Format data properly for Tabula-8B\n    for i in tqdm(range(0, len(df_batch), batch_size), desc=\"Predicting\"):\n        batch_df = df_batch.iloc[i:i+batch_size]\n        \n        # Format each row as CSV for Tabula-8B\n        prompts = []\n        for _, row in batch_df.iterrows():\n            # Create CSV format for single prediction\n            row_df = pd.DataFrame([row])\n            row_df = row_df.drop(columns=['cause_of_death'], errors='ignore')\n            \n            prompt = (\n                \"Task: Predict cause of death from verbal autopsy data.\\n\\n\"\n                \"Data (CSV format):\\n\"\n                f\"{row_df.to_csv(index=False)}\\n\"\n                \"Predict the cause of death. Common causes include: \"\n                \"TB, AIDS/HIV, Malaria, Cardiovascular disease, Respiratory infections, \"\n                \"Road traffic accidents, Suicide, Homicide, Maternal conditions.\\n\\n\"\n                \"Cause of death:\"\n            )\n            prompts.append(prompt)\n        \n        # Tokenize batch\n        inputs = tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=2048  # Increased for CSV data\n        ).to(device)\n        \n        # Generate predictions\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=20,\n                temperature=0.1,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id\n            )\n        \n        # Decode predictions\n        for j, output in enumerate(outputs):\n            generated = tokenizer.decode(\n                output[inputs['input_ids'][j].shape[0]:],\n                skip_special_tokens=True\n            ).strip()\n            \n            # Clean up prediction\n            cause = generated.split('\\n')[0].strip()\n            if ':' in cause:\n                cause = cause.split(':')[-1].strip()\n            \n            # Remove any quotes or extra formatting\n            cause = cause.replace('\"', '').replace(\"'\", '').strip()\n            \n            predictions.append(cause if cause else \"Unknown\")\n    \n    return predictions\n\ndef main(sample_size=100):\n    \"\"\"Run complete pipeline on GPU with CSV formatting.\"\"\"\n    \n    # Load preprocessed data\n    print(\"\\n\" + \"=\"*60)\n    print(\"Loading preprocessed data...\")\n    print(\"=\"*60)\n    \n    # Check if preprocessed data exists\n    processed_path = 'data/preprocessed/phmrc_processed.csv'\n    if not Path(processed_path).exists():\n        print(\"Preprocessed data not found. Running preprocessing...\")\n        preprocessor = PHMRCPreprocessor()\n        df_raw = preprocessor.load_data('data/raw/PHMRC/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv')\n        df_processed = preprocessor.preprocess(df_raw, keep_target=True)\n        Path('data/preprocessed').mkdir(parents=True, exist_ok=True)\n        df_processed.to_csv(processed_path, index=False)\n    else:\n        df_processed = pd.read_csv(processed_path)\n    \n    print(f\"Loaded {len(df_processed)} records with {df_processed.shape[1]} columns\")\n    \n    # Sample data for testing\n    sample_df = df_processed.sample(n=min(sample_size, len(df_processed)), random_state=42)\n    print(f\"\\nUsing {len(sample_df)} samples for prediction\")\n    \n    # Initialize formatter\n    formatter = TabulaVAFormatter()\n    \n    # Load model\n    print(\"\\n\" + \"=\"*60)\n    print(\"Loading Tabula-8B model...\")\n    print(\"=\"*60)\n    model, tokenizer = load_model_gpu()\n    \n    # Run predictions\n    print(\"\\n\" + \"=\"*60)\n    print(\"Running predictions on GPU...\")\n    print(\"=\"*60)\n    predictions = predict_batch_gpu(model, tokenizer, sample_df, formatter, batch_size=4)\n    \n    # Save results\n    results_df = sample_df.copy()\n    results_df['predicted_cause'] = predictions\n    \n    output_path = f'predictions_gpu_{sample_size}.csv'\n    \n    # Select key columns for output\n    output_columns = ['age', 'gender', 'cause_of_death', 'predicted_cause']\n    # Add a few key symptoms if available\n    symptom_cols = ['fever', 'cough', 'difficulty_breathing', 'chest_pain']\n    for col in symptom_cols:\n        if col in results_df.columns:\n            output_columns.append(col)\n    \n    results_df[output_columns].to_csv(output_path, index=False)\n    \n    # Calculate accuracy\n    if 'cause_of_death' in results_df.columns:\n        # Normalize for comparison\n        true_causes = results_df['cause_of_death'].str.lower().str.strip()\n        pred_causes = results_df['predicted_cause'].str.lower().str.strip()\n        \n        # Check for exact matches\n        correct = sum(1 for true, pred in zip(true_causes, pred_causes)\n                     if true == pred or true in pred or pred in true)\n        accuracy = correct / len(predictions)\n        print(f\"\\n✅ Accuracy: {accuracy:.2%} ({correct}/{len(predictions)})\")\n        \n        # Show some examples\n        print(\"\\nSample predictions:\")\n        for i in range(min(5, len(results_df))):\n            print(f\"  True: {results_df.iloc[i]['cause_of_death']}\")\n            print(f\"  Pred: {results_df.iloc[i]['predicted_cause']}\")\n            print()\n    \n    print(f\"✅ Results saved to {output_path}\")\n    return results_df\n\nif __name__ == \"__main__\":\n    import sys\n    sample_size = int(sys.argv[1]) if len(sys.argv) > 1 else 100\n    results = main(sample_size)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 6: Download Model and Run Predictions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# First, run the preprocessing step if not already done\n!python -c \"from src.data.preprocessor import PHMRCPreprocessor; print('Preprocessor imported successfully')\"\n\n# Run predictions with GPU acceleration and CSV formatting\n# This will download the model on first run (~16GB)\n!python run_colab.py 50  # Process 50 samples"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 7: Analyze Results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load and analyze results\nimport pandas as pd\nimport numpy as np\n\n# Load prediction results\nresults_df = pd.read_csv('predictions_gpu_50.csv')\n\nprint(\"=\"*60)\nprint(\"PREDICTION RESULTS ANALYSIS\")\nprint(\"=\"*60)\n\n# Display sample predictions\nprint(\"\\nSample Predictions (first 10):\")\nprint(\"-\"*50)\nfor i in range(min(10, len(results_df))):\n    row = results_df.iloc[i]\n    print(f\"Record {i+1}:\")\n    print(f\"  Age: {row['age']}, Gender: {row['gender']}\")\n    print(f\"  Actual: {row['cause_of_death']}\")\n    print(f\"  Predicted: {row['predicted_cause']}\")\n    print()\n\n# Calculate accuracy metrics\ndef normalize_cause(cause):\n    \"\"\"Normalize cause names for comparison.\"\"\"\n    if pd.isna(cause):\n        return \"unknown\"\n    return str(cause).lower().strip().replace('_', ' ')\n\nresults_df['actual_normalized'] = results_df['cause_of_death'].apply(normalize_cause)\nresults_df['predicted_normalized'] = results_df['predicted_cause'].apply(normalize_cause)\n\n# Exact match accuracy\nexact_matches = (results_df['actual_normalized'] == results_df['predicted_normalized']).sum()\naccuracy = exact_matches / len(results_df)\n\nprint(\"=\"*60)\nprint(\"ACCURACY METRICS\")\nprint(\"=\"*60)\nprint(f\"Total samples: {len(results_df)}\")\nprint(f\"Exact matches: {exact_matches}\")\nprint(f\"Accuracy: {accuracy:.2%}\")\n\n# Confusion matrix for top causes\nfrom collections import Counter\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TOP CAUSES DISTRIBUTION\")\nprint(\"=\"*60)\n\nactual_counts = Counter(results_df['actual_normalized'])\npredicted_counts = Counter(results_df['predicted_normalized'])\n\nprint(\"\\nTop 10 Actual Causes:\")\nfor cause, count in actual_counts.most_common(10):\n    print(f\"  {cause}: {count} ({count/len(results_df)*100:.1f}%)\")\n\nprint(\"\\nTop 10 Predicted Causes:\")\nfor cause, count in predicted_counts.most_common(10):\n    print(f\"  {cause}: {count} ({count/len(results_df)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 8: Download Results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Download the results file\nfrom google.colab import files\n\nprint(\"Downloading prediction results...\")\nfiles.download('predictions_gpu_50.csv')\nprint(\"✅ Results downloaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 9: Run Full Dataset (Optional)\n\n⚠️ **Warning**: Processing the full dataset (7841 records) will take ~15-30 minutes even on GPU"
  },
  {
   "cell_type": "code",
   "source": "# Run on full dataset (optional - takes 15-30 minutes)\n# Uncomment the line below to process all records\n\n# !python run_colab.py 7841  # Process full dataset\n\n# Alternative: Process in smaller batches\n# !python run_colab.py 500   # Process 500 samples\n# !python run_colab.py 1000  # Process 1000 samples",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}