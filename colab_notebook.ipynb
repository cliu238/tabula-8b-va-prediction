{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabula-8B VA Cause of Death Prediction on Google Colab\n",
    "\n",
    "This notebook runs the Tabula-8B model on Google Colab with GPU acceleration for fast inference.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better)\n",
    "- PHMRC dataset file\n",
    "- ~20GB free space for model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup GPU Runtime\n",
    "\n",
    "**IMPORTANT**: Before running this notebook:\n",
    "1. Go to `Runtime` → `Change runtime type`\n",
    "2. Select `GPU` as Hardware accelerator (T4 or better)\n",
    "3. Click `Save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"❌ No GPU detected! Please enable GPU runtime:\")\n",
    "    print(\"   Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/cliu238/tabula-8b-va-prediction.git\n",
    "%cd tabula-8b-va-prediction\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers torch accelerate datasets pandas numpy scikit-learn tqdm python-dotenv pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload PHMRC Data\n",
    "\n",
    "Upload your PHMRC CSV file when prompted. The file should be named:\n",
    "`IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data/raw/PHMRC', exist_ok=True)\n",
    "\n",
    "print(\"Please upload the PHMRC adult dataset CSV file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded file to correct location\n",
    "for filename in uploaded.keys():\n",
    "    if 'ADULT' in filename:\n",
    "        !mv \"{filename}\" data/raw/PHMRC/\n",
    "        print(f\"✅ Moved {filename} to data/raw/PHMRC/\")\n",
    "\n",
    "# Verify file exists\n",
    "!ls -la data/raw/PHMRC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create GPU-Optimized Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_colab.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Colab-optimized script for Tabula-8B VA prediction with GPU support\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.data.preprocessor import PHMRCPreprocessor\n",
    "from src.data.serializer import VADataSerializer\n",
    "\n",
    "def load_model_gpu():\n",
    "    \"\"\"Load Tabula-8B with GPU optimization.\"\"\"\n",
    "    print(\"Loading Tabula-8B model on GPU...\")\n",
    "    \n",
    "    model_name = \"mlfoundations/tabula-8b\"\n",
    "    \n",
    "    # Load with GPU and half precision for speed\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"✅ Model loaded on {torch.cuda.get_device_name(0)}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_batch_gpu(model, tokenizer, texts, batch_size=8):\n",
    "    \"\"\"Run batch predictions on GPU.\"\"\"\n",
    "    predictions = []\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        prompts = [\n",
    "            f\"Based on the following patient information, predict the most likely cause of death.\\n\\n\"\n",
    "            f\"Patient: {text}\\n\\n\"\n",
    "            f\"Cause of death:\"\n",
    "            for text in batch\n",
    "        ]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode predictions\n",
    "        for j, output in enumerate(outputs):\n",
    "            generated = tokenizer.decode(\n",
    "                output[inputs['input_ids'][j].shape[0]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            # Clean up prediction\n",
    "            cause = generated.split('\\n')[0].strip()\n",
    "            if ':' in cause:\n",
    "                cause = cause.split(':')[-1].strip()\n",
    "            \n",
    "            predictions.append(cause if cause else \"Unknown\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def main(sample_size=100):\n",
    "    \"\"\"Run complete pipeline on GPU.\"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    preprocessor = PHMRCPreprocessor()\n",
    "    df = preprocessor.load_data('data/raw/PHMRC/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv')\n",
    "    \n",
    "    # Sample data\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    processed_df = preprocessor.preprocess(sample_df)\n",
    "    \n",
    "    # Serialize to text\n",
    "    print(\"\\nSerializing patient records...\")\n",
    "    serializer = VADataSerializer(verbose=False)\n",
    "    texts = serializer.serialize_batch(processed_df, show_progress=True)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading Tabula-8B model...\")\n",
    "    print(\"=\"*60)\n",
    "    model, tokenizer = load_model_gpu()\n",
    "    \n",
    "    # Run predictions\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Running predictions on GPU...\")\n",
    "    print(\"=\"*60)\n",
    "    predictions = predict_batch_gpu(model, tokenizer, texts, batch_size=8)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = processed_df.copy()\n",
    "    results_df['predicted_cause'] = predictions\n",
    "    \n",
    "    output_path = f'predictions_gpu_{sample_size}.csv'\n",
    "    results_df[['age', 'gender', 'cause_of_death', 'predicted_cause']].to_csv(output_path, index=False)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    if 'cause_of_death' in results_df.columns:\n",
    "        correct = sum(1 for true, pred in zip(results_df['cause_of_death'], predictions)\n",
    "                     if true.lower() == pred.lower())\n",
    "        accuracy = correct / len(predictions)\n",
    "        print(f\"\\n✅ Accuracy: {accuracy:.2%} ({correct}/{len(predictions)})\")\n",
    "    \n",
    "    print(f\"\\n✅ Results saved to {output_path}\")\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sample_size = int(sys.argv[1]) if len(sys.argv) > 1 else 100\n",
    "    results = main(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Model and Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions with GPU acceleration\n",
    "# This will download the model on first run (~16GB)\n",
    "!python run_colab.py 50  # Process 50 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results = pd.read_csv('predictions_gpu_50.csv')\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"Sample Predictions:\")\n",
    "print(results[['age', 'gender', 'cause_of_death', 'predicted_cause']].head(10))\n",
    "\n",
    "# Calculate accuracy by cause\n",
    "if 'cause_of_death' in results.columns:\n",
    "    results['correct'] = results['cause_of_death'].str.lower() == results['predicted_cause'].str.lower()\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {results['correct'].mean():.2%}\")\n",
    "    \n",
    "    # Accuracy by top causes\n",
    "    cause_counts = results.groupby('cause_of_death').agg({\n",
    "        'correct': ['sum', 'count', 'mean']\n",
    "    }).round(3)\n",
    "    cause_counts.columns = ['Correct', 'Total', 'Accuracy']\n",
    "    cause_counts = cause_counts.sort_values('Total', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nAccuracy by Top Causes:\")\n",
    "    print(cause_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the predictions file\n",
    "files.download('predictions_gpu_50.csv')\n",
    "print(\"✅ Results downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run Full Dataset (Optional)\n",
    "\n",
    "⚠️ **Warning**: Processing the full dataset (7841 records) will take ~15-30 minutes even on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run on full dataset\n",
    "# !python run_colab.py 7841"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}